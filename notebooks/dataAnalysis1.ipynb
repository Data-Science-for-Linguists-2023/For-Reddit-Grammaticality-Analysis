{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cfa1dfb",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502a395",
   "metadata": {},
   "source": [
    "This notebook is ***NEW CONTINUING*** from previous notebooks in the [notebooks](https://github.com/Data-Science-for-Linguists-2023/For-Reddit-Grammaticality-Analysis/tree/main/notebooks) folder. Therefore, Before reading through this notebook, I recommend going through [Data Collection](https://github.com/Data-Science-for-Linguists-2023/For-Reddit-Grammaticality-Analysis/blob/main/notebooks/dataCollection.ipynb) and then [Data Organization](https://github.com/Data-Science-for-Linguists-2023/For-Reddit-Grammaticality-Analysis/blob/main/notebooks/dataOrganization.ipynb)\n",
    "\n",
    "**Outline**\n",
    "1. [Setting Up](#Setting-Up)\n",
    "2. [Dataframe Information](#Dataframe-Information)\n",
    "3. [Language Tool Demonstration](#Language-Tool-Demonstration)\n",
    "4. [Setting Up For Analysis](#Setting-Up-for-Analysis)\n",
    "5. [Analysis](#Analysis)\n",
    "6. [Analysis To Be Continued](#Analysis-To-Be-Continued)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f3f11c",
   "metadata": {},
   "source": [
    "In notebooks prior, I have worked on collecting the data from various subreddits and cleaning up that data in a dataframe. In this notebook, I will begin the process of data analysis. Specifically, I will be using [language-tool-python](https://pypi.org/project/language-tool-python/) to parse through the various posts in different subreddits. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd75a0e",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95862800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: language-tool-python in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (2.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from language-tool-python) (4.64.1)\n",
      "Requirement already satisfied: requests in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from language-tool-python) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from requests->language-tool-python) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from requests->language-tool-python) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from requests->language-tool-python) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/camrynsimons/opt/anaconda3/lib/python3.9/site-packages (from requests->language-tool-python) (1.26.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595ab0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with importing our libraries\n",
    "import language_tool_python as ltp  # Using this as a grammaticality parser\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8154c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets up our parsing tool\n",
    "tool = ltp.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25677071",
   "metadata": {},
   "source": [
    "Let's begin by reading in our final CSV files, created in the dataOrganization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e53988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas to read csv files in \n",
    "\n",
    "legalAdvice = pd.read_csv(\"../final-data/finalLegalData.csv\")\n",
    "adulting = pd.read_csv('../final-data/finalAdData.csv')\n",
    "medicine = pd.read_csv('../final-data/finalMedData.csv')\n",
    "highschool = pd.read_csv('../final-data/finalHsData.csv')\n",
    "broadway = pd.read_csv('../final-data/finalBwayData.csv')\n",
    "pittsburgh = pd.read_csv('../final-data/finalPghData.csv')\n",
    "rant = pd.read_csv('../final-data/finalRantData.csv')\n",
    "ccq = pd.read_csv('../final-data/finalCcqData.csv')\n",
    "anime = pd.read_csv('../final-data/finalAnimeData.csv')\n",
    "eli5 = pd.read_csv('../final-data/finalElifData.csv')\n",
    "college = pd.read_csv('../final-data/finalCollegeData.csv')\n",
    "sports = pd.read_csv('../final-data/finalSportsData.csv')\n",
    "crypto = pd.read_csv('../final-data/finalCryptoData.csv')\n",
    "lawyertalk = pd.read_csv('../final-data/finalLawyerData.csv')\n",
    "gaming = pd.read_csv('../final-data/finalGamingData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723121b",
   "metadata": {},
   "source": [
    "Removing the random column from the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f949a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"Unnamed: 0\" column\n",
    "legalAdvice = legalAdvice.drop('Unnamed: 0', axis = 1)\n",
    "adulting = adulting.drop('Unnamed: 0', axis = 1)\n",
    "medicine = medicine.drop('Unnamed: 0', axis = 1)\n",
    "highschool = highschool.drop('Unnamed: 0', axis = 1)\n",
    "broadway = broadway.drop('Unnamed: 0', axis = 1)\n",
    "pittsburgh = pittsburgh.drop('Unnamed: 0', axis = 1)\n",
    "rant = rant.drop('Unnamed: 0', axis = 1)\n",
    "ccq = ccq.drop('Unnamed: 0', axis = 1)\n",
    "anime = anime.drop('Unnamed: 0', axis = 1)\n",
    "eli5 = eli5.drop('Unnamed: 0', axis = 1)\n",
    "college = college.drop('Unnamed: 0', axis = 1)\n",
    "sports = sports.drop('Unnamed: 0', axis = 1)\n",
    "crypto = crypto.drop('Unnamed: 0', axis = 1)\n",
    "lawyertalk = lawyertalk.drop('Unnamed: 0', axis = 1)\n",
    "gaming = gaming.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7cf90",
   "metadata": {},
   "source": [
    "## Dataframe Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984491ff",
   "metadata": {},
   "source": [
    "Let's get a refresher on what the dataframe looks like. Each subreddit dataframe contains:\n",
    "1. Title - The title of the post\n",
    "2. Id - The ID of the post\n",
    "3. Text - The actual contents of the post\n",
    "4. Author - The user that wrote the post\n",
    "5. Number of Comments - The number of comments the post has\n",
    "6. Number of upvotes - The number of upvotes the post has\n",
    "7. Ratio of Upvotes - The ratio of upvotes to downvotes\n",
    "\n",
    "**1, 2 4**: Not super relevant for our purposes right now, mostly to keep track of these posts if they need to be searched on Reddit\n",
    "\n",
    "**3**: Relevant for our purposes right now, will be working with this part shortly\n",
    "\n",
    "**5 - 7**: Not super relevant to our purposes right now, will potentially work with this information in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a9cf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Author</th>\n",
       "      <th>Number of Comments</th>\n",
       "      <th>Number of upvotes</th>\n",
       "      <th>Ratio of Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Big N Discussion - March 19, 2023</td>\n",
       "      <td>11ve46y</td>\n",
       "      <td>Please use this thread to have discussions abo...</td>\n",
       "      <td>CSCQMods</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daily Chat Thread - March 19, 2023</td>\n",
       "      <td>11ve5o1</td>\n",
       "      <td>Please use this thread to chat, have casual di...</td>\n",
       "      <td>CSCQMods</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it acceptable to do lunch 12-1pm at work? A...</td>\n",
       "      <td>11voie0</td>\n",
       "      <td>Asking as a new grad who is trying to understa...</td>\n",
       "      <td>TheCockatoo</td>\n",
       "      <td>214</td>\n",
       "      <td>225</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Open Tech Jobs has increased for 2 c...</td>\n",
       "      <td>11vqmgd</td>\n",
       "      <td>https://www.trueup.io/job-trend\\n\\nThis is a f...</td>\n",
       "      <td>TheCopyPasteLife</td>\n",
       "      <td>46</td>\n",
       "      <td>95</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to enforce good practices in my workplace?</td>\n",
       "      <td>11viy3c</td>\n",
       "      <td>My team doesn't enforce good practices, and my...</td>\n",
       "      <td>Old-Fennel9061</td>\n",
       "      <td>58</td>\n",
       "      <td>149</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>Nerves about starting first SWE Role</td>\n",
       "      <td>11k8to9</td>\n",
       "      <td>I’m graduating from a top CS university this s...</td>\n",
       "      <td>BringMeTheBRBS</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>Reaching out to someone for help with a position</td>\n",
       "      <td>11k87kz</td>\n",
       "      <td>Hello all, to cut to the chase, I was recently...</td>\n",
       "      <td>businessbee89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>Portfolio projects - better to create somethin...</td>\n",
       "      <td>11k7q3y</td>\n",
       "      <td>So I'm getting started creating a project for ...</td>\n",
       "      <td>GroundFallsOnly</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>SWEs in the UK, what is your day-to-day actual...</td>\n",
       "      <td>11k7dz7</td>\n",
       "      <td>We see lots of YouTube videos where young SWEs...</td>\n",
       "      <td>nonbog</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>Switching to a CS path: low-code, Java or Python?</td>\n",
       "      <td>11k52mf</td>\n",
       "      <td>Hi guys, just looking for opinions here. I wor...</td>\n",
       "      <td>coffeeandwomen</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title       Id  \\\n",
       "0                     Big N Discussion - March 19, 2023  11ve46y   \n",
       "1                    Daily Chat Thread - March 19, 2023  11ve5o1   \n",
       "2     Is it acceptable to do lunch 12-1pm at work? A...  11voie0   \n",
       "3     Number of Open Tech Jobs has increased for 2 c...  11vqmgd   \n",
       "4        How to enforce good practices in my workplace?  11viy3c   \n",
       "...                                                 ...      ...   \n",
       "1495               Nerves about starting first SWE Role  11k8to9   \n",
       "1496   Reaching out to someone for help with a position  11k87kz   \n",
       "1497  Portfolio projects - better to create somethin...  11k7q3y   \n",
       "1498  SWEs in the UK, what is your day-to-day actual...  11k7dz7   \n",
       "1499  Switching to a CS path: low-code, Java or Python?  11k52mf   \n",
       "\n",
       "                                                   Text            Author  \\\n",
       "0     Please use this thread to have discussions abo...          CSCQMods   \n",
       "1     Please use this thread to chat, have casual di...          CSCQMods   \n",
       "2     Asking as a new grad who is trying to understa...       TheCockatoo   \n",
       "3     https://www.trueup.io/job-trend\\n\\nThis is a f...  TheCopyPasteLife   \n",
       "4     My team doesn't enforce good practices, and my...    Old-Fennel9061   \n",
       "...                                                 ...               ...   \n",
       "1495  I’m graduating from a top CS university this s...    BringMeTheBRBS   \n",
       "1496  Hello all, to cut to the chase, I was recently...     businessbee89   \n",
       "1497  So I'm getting started creating a project for ...   GroundFallsOnly   \n",
       "1498  We see lots of YouTube videos where young SWEs...            nonbog   \n",
       "1499  Hi guys, just looking for opinions here. I wor...    coffeeandwomen   \n",
       "\n",
       "      Number of Comments  Number of upvotes  Ratio of Upvotes  \n",
       "0                      7                  5              0.73  \n",
       "1                      0                  1              0.60  \n",
       "2                    214                225              0.74  \n",
       "3                     46                 95              0.89  \n",
       "4                     58                149              0.91  \n",
       "...                  ...                ...               ...  \n",
       "1495                   3                  0              0.33  \n",
       "1496                   1                  1              1.00  \n",
       "1497                   4                  5              1.00  \n",
       "1498                   7                  3              0.80  \n",
       "1499                   5                  2              1.00  \n",
       "\n",
       "[1500 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some entries of the ccq(CS Career Questions) dataframe\n",
    "ccq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afd8fc",
   "metadata": {},
   "source": [
    "Now, let's just focus in on one post from that subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf604838",
   "metadata": {},
   "source": [
    "Posts can potentially contain a lot of characters that the grammaticality library may not like. By not removing them, this could lead to unfair results as *technically* these posts are grammatical, they just contain words such as \"Reddit\" or \"Subreddit\". Potentially, as I am working on this analysis, because there are so many cases to consider, I can look at later whether these types of errors fall within the same category so that I could just ignore that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502e0ed",
   "metadata": {},
   "source": [
    "## Language Tool Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ab9b5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"https://www.trueup.io/job-trend\\n\\nThis is a follow up from [last week's post.](https://old.reddit.com/r/cscareerquestions/comments/11odfe7/number_of_open_tech_jobs_has_increased_for_the/) It definitely seems like the market is starting to turn around. I also have anecdotal evidence of my own. Feel free to add yours.\\n\\nPossible risks include reduced lending to startups due to regional bank liquidity. Also another wave of layoffs, like Facebook, but I think that Facebook's layoffs come from a dying business, not an industry-wide concern.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onePost = ccq['Text'][3]\n",
    "onePost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e12ef",
   "metadata": {},
   "source": [
    "Essentially, for each error, it will put the entire error in parentheses, preceded by \"Match\". The tool says what type of error it is, provides a bit of an explanation, and then suggests fixes. For the post above, there are two errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecbb3df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match({'ruleId': 'VERB_NOUN_CONFUSION', 'message': 'When ‘follow-up’ is used as a noun or modifier, it needs to be hyphenated.', 'replacements': ['follow-up'], 'offsetInContext': 43, 'context': \"...ps://www.trueup.io/job-trend  This is a follow up from [last week's post.](https://old.re...\", 'offset': 43, 'errorLength': 9, 'category': 'COMPOUNDING', 'ruleIssueType': 'uncategorized', 'sentence': \"This is a follow up from [last week's post.](https://old.reddit.com/r/cscareerquestions/comments/11odfe7/number_of_open_tech_jobs_has_increased_for_the/) It definitely seems like the market is starting to turn around.\"}),\n",
       " Match({'ruleId': 'SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 'message': 'A comma may be missing after the conjunctive/linking adverb ‘Also’.', 'replacements': ['Also,'], 'offsetInContext': 43, 'context': '...tartups due to regional bank liquidity. Also another wave of layoffs, like Facebook,...', 'offset': 401, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"Also another wave of layoffs, like Facebook, but I think that Facebook's layoffs come from a dying business, not an industry-wide concern.\"})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the tool to see what it will give us for the post above\n",
    "oneCcqPost = tool.check(onePost)\n",
    "oneCcqPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e886561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB_NOUN_CONFUSION'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This syntax will be important... will allow us to directly access the errors of each paragraph\n",
    "oneCcqPost[0].ruleId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3947a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneCcqPost[1].ruleId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8ba8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Please use this thread to chat, have casual discussions, and ask casual questions. Moderation will be light, but don't be a jerk.\\n\\nThis thread is posted **every day at midnight PST**. Previous Daily Chat Threads can be found [here](https://www.reddit.com/r/cscareerquestions/search?q=Daily+Chat+Thread&restrict_sr=on&sort=new&t=all).\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onePost = ccq['Text'][1]\n",
    "onePost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15db6c",
   "metadata": {},
   "source": [
    "If the post has no errors, it will run silently like below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41592037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneCcqPost = tool.check(onePost)\n",
    "oneCcqPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b4a56fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Crossposting from r/AskAcademia \\\\- thought I could get some useful knowledge from a different group of folks.\\n\\nI'm in a really tough situation and have been feeling acutely ill over making a decision. I would be really grateful for some advice.\\n\\nI graduated from a prestigious university with a bachelors in CS last spring, and have been working and making a really comfortable salary in a big tech company (return offer on a summer 2021 internship). I had worked with an assistant professor on ML research for the last two years and had a good and productive time; the professor really believed in me, and towards the last semester, suggested that I apply for PhD programs in that area.\\n\\nI did not believe that I could get into any PhD programs this cycle, as this particular subfield is extremely competitive, and so figured that I would not seriously decide whether or not to do a PhD unless I get in. I only targeted very competitive places and applied to my undergrad's fully-funded, research-focused MS program (most likely I'd be working with the same advisor again) as a backup.\\n\\nAll my results came in, and I have gotten into a number of very good PhD programs, as well as the MSCS backup. Among my PhD offers, one is from a non-top 4 CS school which ranks among the top 3 of my subfield, and the advisor that I got is very, very good. The prospective PhD advisor is extremely well-connected and super nice, and my undergraduate advisor was previously affiliated with their group and could not have anything more positive to say.\\n\\nAmong the academic folks whom I've spoken to, the overwhelming consensus is that I should just go to the state school and work with that professor. And I feel like if I were to pursue a PhD, I would definitely take up the offer. But I've been getting very cold feet about the whole business of doing a PhD; my undergrad experience was very stressful and I just could not imagine 6 more years of lowish pay (that particular school gives the lowest stipend, despite being located in a fairly expensive city). I have zero interest in becoming a professor and would ideally like to be in research-oriented roles in the industry, and that's why I'm seriously considering the MSCS backup. At this point, I'm pretty set on going back to school, given the turbulence and ongoing layoffs in the tech industry.\\n\\nHere are some direct concerns about going into the PhD:\\n\\n1. Earning a livable wage and having savings are definitely very important to me and I can't justify walking away from an six-figure annual salary and living for 5-6 years on a $30K-ish PhD stipend. Comparatively, the MSCS stipend is on the higher end (\\\\~$50K) with a good exit opportunity to industry after 2 years.\\n2. I love parts of the research process, but I won't deny that it was highly stressful and I came out of undergrad feeling extremely burnt out, with horrible work-life balance and a lot of unprocessed emotional issues. My quality of life has greatly improved since getting into a normal 9-5, but I just can't imagine 6 more years of going through that again.\\n3. Maybe the one slightly negative thing (and it's not a big issue) is that the PhD advisor is pretty hands-off given that they already have tenure and are affiliated with some industry connections. I am definitely not a very skilled researcher and would need a lot of hands-on guidance, especially in the first few years. The group is very nice, with senior students, but for the most part it seems decentralized (some people are remote / WFH), so I worry that I wouldn't be getting enough mentorship.\\n4. Not a huge factor, but something I'd have to mentally come to terms with: the PhD program itself has incredible prestige within the subarea, but the school's lay prestige is much lower than my undergrad's.\\n\\nThe primary reason for moving forward with the PhD is that this is really a once-in-a-lifetime opportunity; this advisor is the dream advisor for many other applicants and I feel that I would not get as good of an offer again were I to decide later on in life to pursue a PhD. I am also feeling extremely pressured by a lot of people in academia to just go do the PhD and not even consider the MSCS (not a good feeling).\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onePost = ccq['Text'][22]\n",
    "onePost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679ef2f",
   "metadata": {},
   "source": [
    "As seen below however, words such as 'r/AskAcademia' are marked as a spelling mistake. This is problematic, as a lot of posts refer to r/subredditname, but that does not make them ungrammatical in the perspective that I wish to look at it. This poses a potential issue with the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f3017e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Cross posting'], 'offsetInContext': 0, 'context': 'Crossposting from r/AskAcademia \\\\- thought I could g...', 'offset': 0, 'errorLength': 12, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Crossposting from r/AskAcademia \\\\- thought I could get some useful knowledge from a different group of folks.'}),\n",
       " Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['Academia'], 'offsetInContext': 20, 'context': 'Crossposting from r/AskAcademia \\\\- thought I could get some useful know...', 'offset': 20, 'errorLength': 11, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': 'Crossposting from r/AskAcademia \\\\- thought I could get some useful knowledge from a different group of folks.'}),\n",
       " Match({'ruleId': 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'message': 'Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.', 'replacements': ['Furthermore, I', 'Likewise, I', 'Not only that, but I'], 'offsetInContext': 43, 'context': '...ld be really grateful for some advice.  I graduated from a prestigious university...', 'offset': 246, 'errorLength': 1, 'category': 'STYLE', 'ruleIssueType': 'style', 'sentence': 'I graduated from a prestigious university with a bachelors in CS last spring, and have been working and making a really comfortable salary in a big tech company (return offer on a summer 2021 internship).'}),\n",
       " Match({'ruleId': 'A_BACHELORS_IN', 'message': 'It seems that a possessive apostrophe is missing.', 'replacements': [\"bachelor's\", \"bachelor's degree\"], 'offsetInContext': 43, 'context': '...ed from a prestigious university with a bachelors in CS last spring, and have been workin...', 'offset': 295, 'errorLength': 9, 'category': 'GRAMMAR', 'ruleIssueType': 'grammar', 'sentence': 'I graduated from a prestigious university with a bachelors in CS last spring, and have been working and making a really comfortable salary in a big tech company (return offer on a summer 2021 internship).'}),\n",
       " Match({'ruleId': 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'message': 'Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.', 'replacements': ['Furthermore, I', 'Likewise, I', 'Not only that, but I'], 'offsetInContext': 43, 'context': '...urn offer on a summer 2021 internship). I had worked with an assistant professor ...', 'offset': 451, 'errorLength': 1, 'category': 'STYLE', 'ruleIssueType': 'style', 'sentence': 'I had worked with an assistant professor on ML research for the last two years and had a good and productive time; the professor really believed in me, and towards the last semester, suggested that I apply for PhD programs in that area.'}),\n",
       " Match({'ruleId': 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'message': 'Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.', 'replacements': ['Furthermore, I', 'Likewise, I', 'Not only that, but I'], 'offsetInContext': 43, 'context': '...I apply for PhD programs in that area.  I did not believe that I could get into a...', 'offset': 689, 'errorLength': 1, 'category': 'STYLE', 'ruleIssueType': 'style', 'sentence': 'I did not believe that I could get into any PhD programs this cycle, as this particular subfield is extremely competitive, and so figured that I would not seriously decide whether or not to do a PhD unless I get in.'}),\n",
       " Match({'ruleId': 'WHETHER', 'message': 'Consider shortening this phrase to just “whether”. It is correct though if you mean ‘regardless of whether’.', 'replacements': ['whether'], 'offsetInContext': 43, 'context': '...gured that I would not seriously decide whether or not to do a PhD unless I get in. I only tar...', 'offset': 861, 'errorLength': 14, 'category': 'REDUNDANCY', 'ruleIssueType': 'style', 'sentence': 'I did not believe that I could get into any PhD programs this cycle, as this particular subfield is extremely competitive, and so figured that I would not seriously decide whether or not to do a PhD unless I get in.'}),\n",
       " Match({'ruleId': 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'message': 'Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.', 'replacements': ['Furthermore, I', 'Likewise, I', 'Not only that, but I'], 'offsetInContext': 43, 'context': '...her or not to do a PhD unless I get in. I only targeted very competitive places a...', 'offset': 905, 'errorLength': 1, 'category': 'STYLE', 'ruleIssueType': 'style', 'sentence': \"I only targeted very competitive places and applied to my undergrad's fully-funded, research-focused MS program (most likely I'd be working with the same advisor again) as a backup.\"}),\n",
       " Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': [', and'], 'offsetInContext': 43, 'context': '... undergrad experience was very stressful and I just could not imagine 6 more years o...', 'offset': 1889, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"But I've been getting very cold feet about the whole business of doing a PhD; my undergrad experience was very stressful and I just could not imagine 6 more years of lowish pay (that particular school gives the lowest stipend, despite being located in a fairly expensive city).\"}),\n",
       " Match({'ruleId': 'EN_A_VS_AN', 'message': 'Use “a” instead of ‘an’ if the following word doesn’t start with a vowel sound, e.g. ‘a sentence’, ‘a university’.', 'replacements': ['a'], 'offsetInContext': 43, 'context': \"...e and I can't justify walking away from an six-figure annual salary and living for...\", 'offset': 2518, 'errorLength': 2, 'category': 'MISC', 'ruleIssueType': 'misspelling', 'sentence': \"Earning a livable wage and having savings are definitely very important to me and I can't justify walking away from an six-figure annual salary and living for 5-6 years on a $30K-ish PhD stipend.\"}),\n",
       " Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': [', and'], 'offsetInContext': 43, 'context': \"... won't deny that it was highly stressful and I came out of undergrad feeling extreme...\", 'offset': 2802, 'errorLength': 4, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': \"I love parts of the research process, but I won't deny that it was highly stressful and I came out of undergrad feeling extremely burnt out, with horrible work-life balance and a lot of unprocessed emotional issues.\"})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneCcqPost = tool.check(onePost)\n",
    "oneCcqPost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43428eb1",
   "metadata": {},
   "source": [
    "However, if we use regex to substitute out \"reddit\" in strings, the parser does not mark this as an error. This may be helpful in my pursuit of finding meaninful grammatical errors in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a3fc46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a Reddit post.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"This is a Reddit post.\"\n",
    "test = re.sub(r\"[rR]\\/\\S+\", \"--\", test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81f7749a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneTestPost = tool.check(test)\n",
    "oneTestPost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70e770",
   "metadata": {},
   "source": [
    "## Setting Up for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e734af2",
   "metadata": {},
   "source": [
    "Now, for analysis purposes, we want to grab all of the CSV files \"Text\" columns and put them into a list for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bcadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of 'Text' values\n",
    "\n",
    "allSportsVals = list(sports['Text'].values)\n",
    "allLegalVals = list(legalAdvice['Text'].values)\n",
    "allAdultVals = list(adulting['Text'].values)\n",
    "allMedVals = list(medicine['Text'].values)\n",
    "allHsVals = list(highschool['Text'].values)\n",
    "allBwayVals = list(broadway['Text'].values)\n",
    "allPghVals = list(pittsburgh['Text'].values)\n",
    "allRantVals = list(rant['Text'].values)\n",
    "allCcqVals = list(ccq['Text'].values)\n",
    "allAnimeVals = list(anime['Text'].values)\n",
    "allEli5Vals = list(eli5['Text'].values)\n",
    "allCollegeVals = list(college['Text'].values)\n",
    "allCryptoVals = list(crypto['Text'].values)\n",
    "allLawyerVals = list(lawyertalk['Text'].values)\n",
    "allGamingVals = list(gaming['Text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9c619",
   "metadata": {},
   "source": [
    "Now, we want to append the output of the language tool on each item in the list containing text from each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76501bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists\n",
    "sportsErrors = []\n",
    "legalErrors = []\n",
    "adultErrors = []\n",
    "medErrors = []\n",
    "hsErrors = []\n",
    "bwayErrors = []\n",
    "pghErrors = []\n",
    "rantErrors = []\n",
    "ccqErrors = []\n",
    "animeErrors = []\n",
    "eli5Errors = []\n",
    "collegeErrors = []\n",
    "cryptoErrors = []\n",
    "lawyerErrors = [] \n",
    "gamingErrors = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0733db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "# Appending the output of the language tool to a list\n",
    "[sportsErrors.append(tool.check(x)) for x in allSportsVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e92d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[legalErrors.append(tool.check(x)) for x in allLegalVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b484394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[adultErrors.append(tool.check(x)) for x in allAdultVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdae15e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[medErrors.append(tool.check(x)) for x in allMedVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "870030be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[hsErrors.append(tool.check(x)) for x in allHsVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b512561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[bwayErrors.append(tool.check(x)) for x in allBwayVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7281fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[pghErrors.append(tool.check(x)) for x in allPghVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e19f7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[rantErrors.append(tool.check(x)) for x in allRantVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2993968f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[ccqErrors.append(tool.check(x)) for x in allCcqVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb6e05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[animeErrors.append(tool.check(x)) for x in allAnimeVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "feb0c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[eli5Errors.append(tool.check(x)) for x in allEli5Vals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6092feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[collegeErrors.append(tool.check(x)) for x in allCollegeVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0406aa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[cryptoErrors.append(tool.check(x)) for x in allCryptoVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db47f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[lawyerErrors.append(tool.check(x)) for x in allLawyerVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c71cfffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing\n"
     ]
    }
   ],
   "source": [
    "[gamingErrors.append(tool.check(x)) for x in allGamingVals]\n",
    "print('Done processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92ce7c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match({'ruleId': 'MD_BASEFORM', 'message': 'The modal verb ‘will’ requires the verb’s base form.', 'replacements': ['become'], 'offsetInContext': 43, 'context': '...-2023 school year: 2022-2023 FAFSA will became available October 1, 2021. Requires 202...', 'offset': 522, 'errorLength': 6, 'category': 'GRAMMAR', 'ruleIssueType': 'grammar', 'sentence': '2022-2023 school year: 2022-2023 FAFSA will became available October 1, 2021.'}),\n",
       " Match({'ruleId': 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'message': 'Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.', 'replacements': [], 'offsetInContext': 43, 'context': '...ir own FSA account, they must use that. If your parent does not have an SSN, they ...', 'offset': 997, 'errorLength': 2, 'category': 'STYLE', 'ruleIssueType': 'style', 'sentence': 'If your parent does not have an SSN, they must print and sign the signature page manually, then mail it in.'}),\n",
       " Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['untamed'], 'offsetInContext': 43, 'context': '...ion (W-2s, tax returns), any records of untaxed income, etc.  * Start the FAFSA! If you...', 'offset': 1219, 'errorLength': 7, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': '* Gather all necessary documents, including bank statements, tax information (W-2s, tax returns), any records of untaxed income, etc.'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just making sure it is formatted correctly\n",
    "\n",
    "collegeErrors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65db41c",
   "metadata": {},
   "source": [
    "For now, I want to focus in on the RuleId(ex: MORFOLOGIK_RULE_EN_US). Let's grab those from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e94b42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# For each subreddit, append the rule id to the list\n",
    "simplifiedColErrors = []\n",
    "[simplifiedColErrors.append(y.ruleId) for x in collegeErrors for y in x]\n",
    "\n",
    "simplifiedSportsErrors = []\n",
    "[simplifiedSportsErrors.append(y.ruleId) for x in sportsErrors for y in x]\n",
    "\n",
    "simplifiedLegalErrors = []\n",
    "[simplifiedLegalErrors.append(y.ruleId) for x in legalErrors for y in x]\n",
    "\n",
    "simplifiedAdultErrors = []\n",
    "[simplifiedAdultErrors.append(y.ruleId) for x in adultErrors for y in x]\n",
    "\n",
    "simplifiedMedErrors = []\n",
    "[simplifiedMedErrors.append(y.ruleId) for x in medErrors for y in x]\n",
    "\n",
    "simplifiedHsErrors = []\n",
    "[simplifiedHsErrors.append(y.ruleId) for x in hsErrors for y in x]\n",
    "\n",
    "simplifiedBwayErrors = []\n",
    "[simplifiedBwayErrors.append(y.ruleId) for x in bwayErrors for y in x]\n",
    "\n",
    "simplifiedPghErrors = []\n",
    "[simplifiedPghErrors.append(y.ruleId) for x in pghErrors for y in x]\n",
    "\n",
    "simplifiedRantErrors = []\n",
    "[simplifiedRantErrors.append(y.ruleId) for x in rantErrors for y in x]\n",
    "\n",
    "simplifiedCcqErrors = []\n",
    "[simplifiedCcqErrors.append(y.ruleId) for x in ccqErrors for y in x]\n",
    "\n",
    "simplifiedAnimeErrors = []\n",
    "[simplifiedAnimeErrors.append(y.ruleId) for x in animeErrors for y in x]\n",
    "\n",
    "simplifiedEli5Errors = []\n",
    "[simplifiedEli5Errors.append(y.ruleId) for x in eli5Errors for y in x]\n",
    "\n",
    "simplifiedCryptoErrors = []\n",
    "[simplifiedCryptoErrors.append(y.ruleId) for x in cryptoErrors for y in x]\n",
    "\n",
    "simplifiedLawyerErrors = []\n",
    "[simplifiedLawyerErrors.append(y.ruleId) for x in lawyerErrors for y in x]\n",
    "\n",
    "simplifiedGamingErrors = []\n",
    "[simplifiedGamingErrors.append(y.ruleId) for x in gamingErrors for y in x]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52b77ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WHOSE_DT',\n",
       " 'MORFOLOGIK_RULE_EN_US',\n",
       " 'MORFOLOGIK_RULE_EN_US',\n",
       " 'MORFOLOGIK_RULE_EN_US',\n",
       " 'LIFE_COMPOUNDS',\n",
       " 'OVER_COMPOUNDS',\n",
       " 'ENGLISH_WORD_REPEAT_BEGINNING_RULE',\n",
       " 'WHITESPACE_RULE',\n",
       " 'MISSING_HYPHEN',\n",
       " 'WHITESPACE_RULE']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplifiedGamingErrors[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50651d0e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da3ab3",
   "metadata": {},
   "source": [
    "Ok that's it! Thats the entire project! Anime clearly is the most ungrammatical subreddit out of the 15. Just kidding...time to look further into all of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a322b8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6535\n",
      "24724\n",
      "7210\n",
      "8008\n",
      "4957\n",
      "8321\n",
      "5391\n",
      "3040\n",
      "10431\n",
      "5884\n",
      "14851\n",
      "1791\n",
      "13602\n",
      "3943\n",
      "6238\n"
     ]
    }
   ],
   "source": [
    "# Printing the length of each list, which is the number of errors in each subreddit\n",
    "\n",
    "print(len(simplifiedColErrors))\n",
    "print(len(simplifiedSportsErrors)) \n",
    "print(len(simplifiedLegalErrors)) \n",
    "print(len(simplifiedAdultErrors ))\n",
    "print(len(simplifiedMedErrors))\n",
    "print(len(simplifiedHsErrors))\n",
    "print(len(simplifiedBwayErrors ))\n",
    "print(len(simplifiedPghErrors ))\n",
    "print(len(simplifiedRantErrors ))\n",
    "print(len(simplifiedCcqErrors ))\n",
    "print(len(simplifiedAnimeErrors))\n",
    "print(len(simplifiedEli5Errors ))\n",
    "print(len(simplifiedCryptoErrors))\n",
    "print(len(simplifiedLawyerErrors))\n",
    "print(len(simplifiedGamingErrors ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c8e5bf",
   "metadata": {},
   "source": [
    "Listed below is all of the possible errors across all of the subreddits. There are a lot of errors, and some of them aren't super clear as to what they are talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23c817e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER_SE', 'MUST_HAVE_TO', 'TAKEAWAY', 'THIS_MISSING_VERB', 'IF_OF', 'WE_LL_WELL', 'STAIRS_COMPOUNDS', 'AFFECT_EFFECT', 'MOST_SOME_OF_NNS', 'THE_BEST_WAY', 'THIRD_WORLD', 'PRP_JJ', 'MASS_AGREEMENT', 'WOULD_BE_JJ_VB', 'FR', 'GO_GERUND', 'TO_TOO', 'DAYTIME', 'LEARN_NNNNS_ON_DO', 'BACHELOR_ABBR', 'SENT_START_ARE_NOT_ARENT_FORMAL', 'ON_COMPOUNDS', 'HAND_FULL_COMPOUND', 'HIGH_END_HYPHEN', 'ALL_OF_THE_SUDDEN', 'HAND_AND_HAND', 'EN_WORD_COHERENCY', 'COMMA_TAG_QUESTION', 'GATHER_UP', 'NOTE_TAKING_HYPHEN', 'LIFE_LIVE', 'BEEN_PART_AGREEMENT', 'CC_NNP_VBP', 'FEWER_LESS', 'WIFI', 'CASTED', 'TH_THORIUM', 'MOST_SUPERLATIVE', 'ROYAL_MAIL', 'PRP_THE', 'IN_OR_WITH_REGARDS_TO_OF', 'LESS_MORE_THEN', 'COMMA_COMPOUND_SENTENCE_3', 'THE_SUPERLATIVE', 'TWO_CONNECTED_MODAL_VERBS', 'DUN_DONT', 'A_COLLECTIVE_OF_NN', 'NOT_SURE_IT_WORKS', 'ACHE_COMPOUNDS', 'RETURN_BACK', 'ALA_MODE', 'WHOS', 'HIPAA', 'PRP_VB', 'FOR_VB', 'DATE_WEEKDAY_WITHOUT_YEAR', 'WORLDS_BEST', 'TO_DO_HYPHEN', 'IT_ITS', 'FOR_GODS_SAKE_COMMA', 'WELL_WILL', 'MID_HYPHEN', 'IN_1990s', 'RITE_WRITE', 'WHAT_SO_EVER', 'TO_ABLE', 'HAVE_A_SHOWER', 'INCORRECT_POSSESSIVE_FORM_AFTER_A_NUMBER', 'TRUE_CRIME_HYPHEN', 'INCORRECT_CONTRACTIONS', 'WE_BE', 'TOILET', 'SECOND_LARGEST_HYPHEN', 'WHOLE_COMPOUNDS', 'IT_VBZ', 'DO_OVER_HYPHEN', 'YOUR_SHOULD', 'WOMAN_WOMEN', 'CANT', 'GITHUB', 'OFF_OF', 'NOW_ARE_THE_TIME', 'DOES_X_HAS', 'LOTS_OF_NN', 'FOCUS_IN', 'FOR_SOMETIME_FOR_SOME_TIME', 'MUCH_COUNTABLE', 'LOSE_LIVES', 'SHIT_SHOW_COMPOUND', 'MY_BE', 'JACK_DANIELS', 'RB_RB_COMMA', 'MANY_TIME', 'WILL_BECOMING', 'DIVERSITY_OF', 'IN_WHO', 'A_INFINITIVE', 'WEN_WE', 'ONCE_AND_A_WHILE', 'DUNNO', 'A_WAS', 'THEIR_S', 'PROOF_COMPOUNDS', 'BARLEY_BARELY', 'MAN_MADE', 'WITHDRAWAL_WITHDRAW', 'EXCEPTION_OF_TO', 'NEE', 'A_NUMBER_NNS', 'VERB_NOUN_CONFUSION', 'NOWADAYS_COMMA', 'ONES', 'CA_FOLLOW_UP', 'WRONG_PRP_AT_SENT_START', 'ONE_HANDED_HYPHEN', 'UPPERCASE_SENTENCE_START', 'SPURIOUS_APOSTROPHE', 'DOSE_DOES', 'A_GOOGLE', 'OVER_SEAS', 'RELY_ON', 'PRP_MD_PRP_MD_COMMA', 'WORDPRESS', 'BACK_AND_FOURTH', 'WHEN_WHERE', 'DEGREE_HYPHEN', 'AS_ADJ_AS', 'ORDER_OF_WORDS_WITH_NOT', 'BACK_COMPOUNDS', 'DID_FOUND_AMBIGUOUS', 'EQUALLY_AS', 'SKY_COMPOUNDS', 'DT_PRP', 'YOUR_YOU_2', 'KIND_OF_A', 'ARTICLE_ADJECTIVE_OF', 'REPEATED_VERBS', 'HAVE_PART_AGREEMENT', 'CURRENCY', 'PREFER_OVER_TO', 'LOWERCASE_MONTHS', 'A_WINDOWS', 'EACH_EVERY_NNS', 'YA_LL', 'FROM_FORM', 'AN_AND', 'PRP_ABLE_TO', 'THEM_SELVES', 'WHO_NOUN', 'WORK_LIFE_BALANCE', 'GROUND_FIRST_FLOOR', 'ALSO_KNOW', 'IN_TACT', 'SENT_AN_EMAIL', 'MUTE_POINT', 'OVER_TIME', 'ARRIVE_ON_AT_THE_BEACH', 'PONZI_SCHEME', 'THAT_SOUND_GREAT', 'DO_VBZ', 'SUPERLATIVE_THAN', 'MD_BE_NON_VBP', 'WRONG_GENITIVE_APOSTROPHE', 'ILL_I_LL', 'WHERE_AS', 'MENTION_ABOUT', 'ALONG_SIDE', 'THAT_VERY_COOL', 'IN_TO_INTO', 'BREATHE_BREATH', 'CAPITALIZATION', 'COLLECTIVE_NOUN_VERB_AGREEMENT_VBP', 'COULD_CARE_LESS', 'DAY_TO_DAY_HYPHEN', 'RACK_BRAIN', 'INDEPENDENTLY_FROM_OF', 'AS_OF_YET', 'WHO_WHOM', 'LUV', 'IS_THERE_ANY_NNS', 'GRADUATE_FROM', 'COMMA_PARENTHESIS_WHITESPACE', 'I_M_MD', 'MISSING_PAST_TENSE', 'THRU', 'ENGLISH_WRONG_WORD_IN_CONTEXT', 'IN_A_TROUBLE', 'TANK_YOU', 'ABOUT_ITS_NN', 'APOS_SPACE_CONTRACTION', 'STEP_BY_STEP_HYPHEN', 'LAYING_AROUND', 'GOT_SHUTDOWN', 'HOLDER_COMPOUNDS', 'ARE_WE_HAVE', 'Y_ALL', 'NNS_THAT_VBZ', 'OUT_SIDE', 'DO_MAKE_PRP_VBG', 'PRP_RESPONSE', 'NN_CD_NN_CD_COMMA', 'PRP_VB_VB_TO', 'A_TO', 'TIRED_ABOUT_OF', 'ADVICE_ADVISE', 'WHOS_ACTUAL', 'PH_D', 'CONFUSION_OF_THEN_THAN', 'CHOOSED', 'NO_COMMA_BEFORE_INDIRECT_QUESTION', 'T_REX', 'LITTLE_BIT', 'GOD_COMMA', 'SEND_PRP_AN_EMAIL', 'A_INSTALL', 'THEN_WHEN_COMMA', 'DOES_NP_VBZ', 'EN_DIACRITICS_REPLACE', 'ASK_THE_QUESTION', 'MISSING_TO_BEFORE_A_VERB', 'OFT_HE', 'GOING_TO_VBD', 'UNIT_SPACE', 'CONFUSION_OF_MANS_MEN', 'ACCOMPANY_WITH', 'LESS_DOLLARSMINUTESHOURS', 'HEARS_YEARS', 'NIT_NOT', 'COME_IN_TO', 'A_BUT', 'MA_MY', 'ROOM_COMPOUNDS', 'IN_THE_MEAN_TIME_PHRASE', 'SHOE_IN', 'DOUBLE_HYPHEN', 'LIMITED_TO', 'CAPITALIZATION_NNP_DERIVED', 'THAN_I', 'LOW_HANGING_FRUIT_HYPHEN', 'CONSEQUENCES_OF_FOR', 'THE_US', 'BALL_COMPOUNDS', 'EXCITED_FOR', 'WEEK_LONG_HYPHEN', 'DAY_TRADE_HYPHEN', 'MOST_COMPARATIVE', 'LOOSE_LOSE', 'DID_BASEFORM', 'THE_DUTCH', 'COPY_PASTE', 'DOUBLE_APOSTROPHE', 'PREPOSITION_VERB', 'ON_EXCEL', 'DOUBLE_PUNCTUATION', 'NUMBERS_IN_WORDS', 'EN_CONTRACTION_SPELLING', 'PRP_PAST_PART', 'ME_BE', 'COMPARISONS_THEN', 'MANY_NN', 'KEEPER_COMPOUNDS', 'WHAT_DID_VBD', 'MORE_A_JJ', 'A_RB_A_JJ_NN', 'PRP_RB_NO_VB', 'LOG_IN', 'ADVERB_VERB_ADVERB_REPETITION', 'I_AM_VB', 'HANDS_ON_HYPHEN', 'PEAK_ATTENTION', 'WORTH_WHILE', 'OTHER_OTHERS', 'BECAUSE_OF_I', 'PICK_UP_COMPOUND', 'COM_COME', 'YOUR_YOU_RE', 'MISSING_NOUN', 'HELP_NP_VBZ', 'YESTERDAY_NIGHT', 'DIS', 'THATS_ITS', 'OF_ANY_OF', 'ROOM_APARTMENT_HYPHEN', 'UNLIKELY_OPENING_PUNCTUATION', 'WHAT_VBZ', 'MOST_EVERYONE', 'VB_A_WHILE', 'SHOULD_BE_DO', 'WERE_ARE', 'BE_NO_VB', 'SINGULAR_VERB_AFTER_THESE_OR_THOSE', 'DIFFERENT_THAN', 'HE_NEED', 'FREE_LANCE', 'CAUSE_BECAUSE', 'COMMA_PERIOD', 'COUNTER_COMPOUNDS', 'HOOD_COMPOUNDS', 'YO_TO', 'FEDEX', 'COUD_T', 'BLU_RAY', 'CONFUSION_OF_RIDE_RID', 'BE_VBP_IN', 'GIVE_ADVISE', 'THE_FRENCH', 'INSIST_ON_GERUND', 'MAY_MANY_MY', 'TOMORROW_POSSESSIVE_APOSTROPHE', 'CAN_I_VBD', 'GET_A_JOB_IN_WITH', 'WAN_T', 'ASK_NO_PREPOSITION', 'THIS_TOOLS', 'PRESIDENTS_DAY_APOSTROPHE', 'A_BACHELORS_IN', 'INTERJECTIONS_PUNCTUATION', 'I_A', 'I_AFRAID', 'TOO_DETERMINER', 'THAT_S_YOU_RE', 'IN_FUTURE', 'CALL_OF_DUTY', 'EXPLAIN_TO', 'ALL_IN_ONE_HYPHEN', 'WITH_OUT', 'VITAMIN_C', 'MISSING_APOSTROPHE_T', 'BUNCH_OF', 'MISSING_COMMA_WITH_TOO', 'EVERYDAY_EVERY_DAY', 'MIS_MISS', 'BE_WILL', 'LITHIUM_ION', 'DID_PAST', 'AIR_BNB', 'SHORT_CUT', 'ADMIT_ENJOY_VB', 'DESPITE_OF', 'PRP_RB_JJ', 'PROGRESSIVE_VERBS', 'BE_CAUSE', 'HAD_HARD', 'WHOLE_LOT', 'A_BIT_OF', 'THERE_MISSING_VERB', 'IN_THE_RECENT_YEARS_IN_RECENT_YEARS', 'THERE_OWN', 'TONGUE_AND_CHEEK', 'CA_RAGS_TO_RICHES', 'THE_NN_AND_THE_NN', 'HAVE_FOLLOWING_NN', 'BAND_AID', 'WOLD_WOULD', 'A_RB_NN', 'WORST_COMES_TO_WORST', 'ER', 'IF_YOU_ANY', 'FOX_NEWS', 'THEMSELF', 'OTHER_WISE_COMPOUND', 'SEA_COMPOUNDS', 'SHORT_COMING', 'HAD_VBP', 'WHO_S_NN_VB', 'ASSASSINS_CREED', 'IF_IS', 'GONNA', 'WAS_IS', 'NON_NONE', 'STARTING_DOING', 'WHITESPACE_RULE', 'APOS_ARE', 'SEND_AN_EMAIL', 'A_MY', 'CONDITIONAL_CLAUSE', 'IN_THE_INTERNET', 'OUTTA', 'THANKS_SENT_END_COMMA', 'CLOSE_SCRUTINY', 'RATHER_THEN', 'WERE_WHERE', 'SMALL_NUMBER_OF', 'NEITHER_NOR', 'THAT_THAN', 'IT_SEAMS', 'AGREEMENT_QUESTION', 'MICROSOFT_PRODUCTS', 'NO_WHERE', 'HEAD_COMPOUNDS', 'FIRST_PERSON_SHOOTER', 'THIS_YEARS_POSSESSIVE_APOSTROPHE', 'BESIDES_BESIDE', 'EN_A_VS_AN', 'HAPPEN_TO', 'ITS_HAS', 'AND_ETC', 'ALL_NN', 'IS_WAS', 'THANKS_YOU', 'HALO_HALLO', 'STREET_LIGHTS_COMPOUND', 'WRITE_UP_HYPHEN', 'A_CD_NNS', 'AFTERMARKET', 'MISSING_COMMA_AFTER_INTRODUCTORY_PHRASE', 'UNITE_STATES', 'IRREGARDLESS', 'LOOSING_EFFORTRECORDSEASON', 'AT_THE_JOB', 'BE_COME', 'STEPS_TO_DO', 'THEE', 'TYPO_FORM_FROM', 'ROLLS_ROYCE', 'IT_IS', 'COULDVE', 'JAPAN', 'THE_JJR_THE_MORE_COMMA', 'UP_AND_COMING_HYPHEN', 'OUT_OF_PLACE', 'WHOSE_DT', 'ALL_GIRLS_HYPHEN', 'SOME_FACULTY', 'COMMA_COMPOUND_SENTENCE', 'APOSTROPHE_IN_DATES', 'FOR_PROFIT_HYPHEN', 'BUY_TWO_GET_ONE_FREE', 'YOUR', 'SPACE_BETWEEN_NUMBER_AND_WORD', 'NEEDS_FIXED', 'HAVE_HAVE', 'AU', 'MISSING_ARTICLE', 'U_TURN', 'COMMA_THANKS', 'GOD_SEND_COMPOUND', 'YOUR_NN', 'ANY_BODY', 'HAVE_TWITTER', 'I_ME', 'THE_ADD_ON', 'GOOGLE_PRODUCTS', 'OVERNIGHT', 'SENTENCE_WHITESPACE', 'COUPLE_OF_TIMES', 'NOT_US1', 'DO_HE_VERB', 'KNOW_NOW', 'MODAL_OF', 'ABLE_TO_PASSIVE', 'R_SYMBOL', 'ENTER_IN', 'ADD_AN_ADDITIONAL', 'CLICK_HYPHEN', 'LEFT_OVER_COMPOUND', 'GOING_TO_JJ', 'AND_THAN', 'WORKER_COMPOUNDS', 'THING_THINK', 'ENROLLED_IN_FOR', 'ARROWS', 'WOLFS', 'IF_VB_PCT', 'BORED_OF', 'LOT_OF', 'APART_A_PART', 'WAN_WANT', 'MISSING_COMMA_WITH_NNP', 'FOR_EVER_US', 'THIS_NNS', 'TOO_TO', 'AND_BUT', 'EXTEND_EXENT', 'YOU_THING', 'INTEREST_FOR_IN', 'LINKEDIN', 'IS_SHOULD', 'ON_OFF_SCREEN_HYPHEN', 'THE_SAME_AS', 'DO_DUE', 'SINGULAR_AGREEMENT_SENT_START', 'WRITER_COMPOUNDS', 'WORK_AROUND_COMPOUND', 'QUESTION_WITHOUT_VERB', 'AWAITING_FOR', 'RELATIVE_CLAUSE_AGREEMENT', 'MISSING_APOSTROPHE', 'EN_SPECIFIC_CASE', 'COLLECTIVE_NOUN_VERB_AGREEMENT_VBD', 'APPSTORE', 'CA_BRAND_NEW', 'THE_FALL_SEASON', 'SUPPER', 'FOR_AWHILE', 'PRP_COMMA', 'LC_AFTER_PERIOD', 'RIGHT_OVER', 'DOESENT', 'PASSED_PAST', 'ONE_STOP_HYPHEN', 'EXITED_EXCITED', 'STORY_HYPHEN', 'MISSING_COMMA_BETWEEN_DAY_AND_YEAR', 'PAYPAL', 'A_LOT_OF_NN', 'GO_TO_HYPHEN', 'YOUR_LOVES_ONES', 'COMMA_PERIOD_CONFUSION', 'WAS_BEEN', 'SO_THEREFORE', 'EVERY_EVER', 'MONTH_HYPHEN', 'APOS_RE', 'WORRY_FOR', 'VERB_APOSTROPHE_S', 'USE_TO_VERB', 'SEE_SEEN', 'PLACE_COMPOUNDS', 'DT_RB_IN', 'EVERY_BODY', 'I_NOT_JJ', 'ENGLISH_WORD_REPEAT_BEGINNING_RULE', 'HOME_COMPOUNDS', 'ANYWAYS', 'EN_SIMPLE_REPLACE', 'COMMA_COMPOUND_SENTENCE_4', 'TH_CENTURY', 'IS_WERE', 'UNDER_COMPOUNDS', 'TOO_EITHER', 'CA_OUT_OF_BODY', 'BE_HAVENT', 'IN_ANYWAY', 'SOMETIME_SOMETIMES', 'MEED_MEET', 'NUMEROUS_DIFFERENT', 'IVE_CONTRACTION', 'IF_WOULD_HAVE_VBN', 'HAVE_RB_HAVE', 'WELL_WISH_HYPHEN', 'ORDINAL_NUMBER_SUFFIX', 'TL_DR', 'TO_FRESH_UP', 'BE_I_BE_GERUND', 'I_MA', 'HAPPY_EASTER', 'TO_NON_BASE', 'BAY_AREA', 'HOW_COMPOUNDS', 'SIDE_COMPOUNDS', 'COLD_COULD', 'YOUR_YOU', 'MATE_COMPOUNDS', 'OM', 'MUCH_NEEDED_HYPHEN', 'LINE_COMPOUNDS', 'LESS_COMPARATIVE', 'NOUN_AROUND_IT', 'THERE_S_MANY', 'STATUE_OF_LIMITATIONS', 'REASON_IS_BECAUSE', 'SIGN_UP_HYPHEN', 'IMMINENT_DOMAIN', 'TODAY_MORNING', 'FRENCH_S', 'SOON_TO_BE_HYPHEN', 'SEEM_SEEN', 'NE', 'BETWEEN_TO_AND', 'OWNER_COMPOUNDS', 'NATION_WIDE', 'NON3PRS_VERB', 'DROP_DOWN', 'ANINFOR_EVERY_DAY', 'DISAPPOINTED_OF', 'VERY_UNIQUE', 'BU', 'T_BONE', 'PAYED', 'IM_I_M', 'KEY_WORDS', 'WHOM_WHO', 'CD_WEEK_S', 'IN_ON_THE_RIGHT_HAND_SIDE', 'DOSNT', 'APPLY_FOR', 'OUT_OF_THE_WAY', 'HE_LIKE', 'REPEATED_CURRENCY', 'PRP_SOOTHS', 'BELIVE_BELIEVE', 'WERE_MD', 'PRP_HAVE_VB', 'ADJECTIVE_IN_ATTRIBUTE', 'ORIGINALLY_BORN_IN', 'CAR_HYPHEN', 'CYBER_COMPOUNDS', 'LEMME', 'NUT_NOT', 'HE_THE', 'ON_IN_THE_AFTERNOON', 'ANTHER', 'ANY_WHERE', 'COMMA_AFTER_A_MONTH', 'EACH_AND_EVERY', 'DRY_ERASE_HYPHEN', 'PIECE_COMPOUNDS', 'OUT_GROW', 'SHOW_COMPOUNDS', 'AUXILIARY_DO_WITH_INCORRECT_VERB_FORM', 'HE_BE', 'EVERY_EACH_SINGULAR', 'MISSING_HYPHEN', 'APPLE_PRODUCTS', 'A_HEADS_UP', 'SUMMER_TIME', 'THANK_IN_ADVANCE', 'THE_IT', 'ATD_VERBS_TO_COLLOCATION', 'HOME_COMPOUNDS_EN_US', 'PLURAL_VERB_AFTER_THIS', 'HOUR_HYPHEN', 'EN_US_SIMPLE_REPLACE', 'THE_SENT_END', 'PRE_AND_POST_NN', 'SAID_SAD', 'COMPARISONS_NNS_THEN', 'CHRISTMAS', 'NO_GO_HYPHEN', 'EVER_NN', 'SOUND_GREAT', 'FRIEND_COMPOUNDS', 'SOME_OF_THE', 'KNOW_IT_ALL_HYPHEN', 'COULDVE_IRREGULAR_VERB', 'ITS_IS', 'MOST_OF_THE_TIMES', 'CLEAN_UP', 'INFORMATIONS', 'SETUP_VERB', 'BE_TO_VBG', 'EVERY_NOW_AND_THEN', 'ANYMORE_ADVERB', 'FILE_EXTENSIONS_CASE', 'MAKE_SINCE', 'IT_SOMETHING', 'AFFORD_VBG', 'SIMPLE_TO_USE_HYPHEN', 'WHAT_TO_VBD', 'YEAR_OLD_HYPHEN', 'NON_ANTI_JJ', 'PERS_PRONOUN_AGREEMENT', 'BY_BUY', 'ANY_MORE', 'FIGURE_HYPHEN', 'CONFUSION_OF_ME_I', 'MASTERS', 'I_LOWERCASE', 'FELLOW_CLASSMATE', 'SIGN_INTO', 'WRONG_APOSTROPHE', 'BARE_BONES_HYPHEN', 'WEEK_HYPHEN', 'STEP_COMPOUNDS', 'AT_IN_THE_KITCHEN', 'IF_THERE', 'QUIET_QUITE', 'NEAR_BY', 'LARGE_NUMBER_OF', 'THERE_WAS_MANY', 'EN_MASS', 'PLUS_MINUS', 'EN_UNPAIRED_BRACKETS', 'PERSON_HYPHEN', 'YOUR_RE', 'NOUN_VERB_CONFUSION', 'ALL_TIME_HYPHEN', 'PRONOUN_NOUN', 'WHATCHA', 'WHERE_MD_VB', 'ABLE_VBP', 'IT_IT', 'ON-GOING', 'THANKS_IN_ADVANCED', 'IN_CHARGE_OF_FROM', 'HE_VERB_AGR', 'COVID_19', 'WRIGHT_WRITE', 'BASE_FORM', 'AN_ANOTHER', 'IN_FACEBOOK', 'OTHER_THEN', 'A_CAPPELLA', 'HOW_DO_I_VB', 'SAMS_CLUB', 'EXTREME_ADJECTIVES', 'LET_IT_INFINITIVE', 'POKEMON', 'LAUGHING_STOCK', 'SOME_TIMES', 'DEPENDENT', 'FIRST_COME_FIRST_SERVED_COMMA', 'WORLD_WIDE', 'VBG_THEYRE', 'NO_SPACE_CLOSING_QUOTE', 'IN_THE_LONG_TERMS', 'OPEN_OFFICE', 'REPEATED_PLEASE', 'IF_VB', 'WHO_VERB', 'SOFTWARES', 'MD_BASEFORM', 'FIRST_OF_ALL', 'I_DONT_DT', 'FEW_OCCASIONS', 'WORD_ESSAY_HYPHEN', 'ET_AL', 'MAD_MADE', 'VBZ_VBD', 'COMFORTABLE_WITH_VBG', 'PRINT_COMPOUNDS', 'LETS_LET', 'EN_SPLIT_WORDS_HYPHEN', 'QUESTION_MARK', 'HOW_YOU_DOING', 'WAY_COMPOUNDS', 'WITH_THE_EXCEPTION_OF', 'THEIR_IS', 'IN_FRONT_OF', 'ARRIVE_NNP', 'STARS_AND_STEPS', 'ON_SKYPE', 'BUILT_IN_HYPHEN', 'TO_TWO', 'AD_NAUSEUM', 'MISSING_TO_BETWEEN_BE_AND_VB', 'IN_JANUARY', 'SCHITTS_CREEK', 'DESCEND_DOWN', 'AM_PM', 'HAY_DAY', 'EN_COMPOUNDS', 'COCA_COLA', 'PAST_EXPERIENCE_MEMORY', 'DAMAGE_OF_TO', 'ADVERB_WORD_ORDER', 'AGREEMENT_SENT_START', 'JIVE_WITH', 'POSSIBILTY_POSSIBLE', 'COUNTLESS_OF', 'SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 'INTERESTED_BY', 'BOTH_AS_WELL_AS', 'ID_CASING', 'PRP_MD_NN', 'ACCEPT_EXCEPT', 'NODT_DOZEN', 'DOS_AND_DONTS', 'SOME_WHERE', 'GAMEBOY', 'THESE_ONES', 'RECOMMENDED_COMPOUNDS', 'HONESTY_HONESTLY', 'WAY_SIDE', 'YOURE', 'WHAT_HOW', 'HANGOUT_VERB', 'DO_YOU_WHAT', 'FINAL_ADVERB_COMMA', 'FED_UP_OF_WITH', 'AWAY_AWAYS', 'CONFUSION_DUE_DO', 'MISSING_COMMA_AFTER_YEAR', 'SOME_NN_VBP', 'WILL_BASED_ON', 'EMPHASIS_EMPHASIZE', 'TWITTER', 'SUBJECT_DROP', 'NO_NOT', 'ALL_OF_THE', 'THERE_THEIR', 'THE_MOST', 'DT_BAIL_OUT', 'SHUTDOWN', 'EVER_DAY', 'ALL_MOST_SOME_OF_NOUN', 'HART_HEART', 'IN_AT_A_PARTY', 'AFFECTS', 'THROUGH_OUT', 'ABBREVIATION_PUNCTUATION', 'OUT_COME', 'THE_THEM', 'HEART_BROKEN_COMPOUND', 'ANY_WAY_TO_VB', 'EACH_OTHERS', 'I_E', 'BASIC_FUNDAMENTALS', 'DUNKIN_DONUTS', 'LIFE_COMPOUNDS', 'I_AS_LOOKING', 'TAKE_AWHILE', 'IN_A_X_MANNER', 'GOOD_IN_AT', 'THE_PUNCT', 'COULD_OF', 'ADVISE_VBG', 'IN_THIS_MOMENT', 'ITS_TO_IT_S', 'PROBLEM_SOLVE_HYPHEN', 'DECISION_MAKING', 'IM_AM', 'PRP_REPITION', 'THE_ARE', 'THE_SOME', 'VERY_KNOWN', 'HELL', 'PRO_RATA', 'MISSING_GENITIVE', 'THE_THEY', 'PIECE_HYPHEN', 'THE_WELSH', 'WERE_VBB', 'WHOS_NN', 'DOWNPAYMENT', 'DETERMINER_GEOGRAPHICAL_WORD', 'ROLL_PLAYER', 'THERE_IS_A_LOT_OF', 'ON_GOING', 'GENERAL_XX', 'PEOPLE_VBZ', 'WANT_TO_NN', 'SIGN_IN', 'LOW_COST_HYPHEN', 'TEN_FOLD', 'WENDYS', 'FURTHER_MORE_COMPOUND', 'GRASPING_FOR_STRAWS', 'MORFOLOGIK_RULE_EN_US', 'MILE_HYPHEN', 'I_NEVER_HAVE_BEEN', 'AGREEMENT_SENT_START_2', 'YOU_R', 'AIR_COMPOUNDS', 'WORTHY_COMPOUNDS', 'OFF_HAND_COMPOUND', 'ETHER_EITHER', 'IN_TERM_OF_PHRASE', 'IN_THE_MOMENT', 'IN_ON_AN_ALBUM', 'THE_HOW', 'DOWN_COMPOUNDS', 'SHIP_COMPOUNDS', 'SLOW_MO', 'ENGLISH_WORD_REPEAT_RULE', 'MONTH_OF_XXXX', 'HUH_COMMA', 'MISSING_COMMA_AFTER_WEEKDAY', 'A_NNS', 'ALREADY_ALL_READY', 'ALSO_OTHER', 'SUPER_COMPOUNDS', 'GIT_GET', 'AIRCRAFTS', 'UP_COMPOUNDS', 'WORKOUT_VERB', 'THEYRE_THEIR', 'IS_VBZ', 'IF_WE_CANT_COMMA', 'DEPEND_ON', 'DO_TO', 'ITS_JJ_NNSNN', 'WASENT', 'SUPPOSE_TO', 'BE_INTEREST_IN', 'DOUBLE_NEGATIVE', 'FILL_OF_WITH', 'OVER_COMPOUNDS', 'DIFFUSE_TENSIONS', 'MAY_BE', 'EYE_COMPOUNDS', 'LOWERCASE_NAMES', 'ONE_PLURAL', 'LOT_S', 'LES_LESS', 'STAND_UP_HYPHEN', 'A_TRIP_TO', 'AS_TIME_PROGRESSED', 'BACK_IN_FORTH', 'ADJECTIVE_ADVERB', 'I_FOR_ONE_VB_COMMA', 'ALLOW_TO', 'WELL_SUITING', 'ABOVE_MENTIONED', 'PER_USER_BASIS_HYPHEN', 'NOW_A_DAYS', 'NONE_THE_LESS', 'MANOR_MANNER', 'WANNA', 'HOW_EVER', 'ALLY_ALLAY', 'SEEN_SEEM', 'RETURN_IN_THE', 'GUILT_TRIP_HYPHEN', 'PLEASE_NOT_THAT', 'MERCEDES_BENZ', 'WAS_THERE_MANY', 'IN_APP_HYPHEN', 'STAND_ALONE_NN', 'COMPARISONS_AS_ADJECTIVE_AS', 'WONT_CONTRACTION', 'THOUGH_THOUGHT', 'WAKE_UP_HYPHEN', 'SUBJECT_VERB_AGREEMENT_PLURAL', 'BUTTON_UP_HYPHEN', 'THERE_RE_MANY', 'YOUTUBE', 'GLAD_WITH_ABOUT', 'PERSPECTIVES_ABOUT_ON', 'FIRST_SERVED', 'ONE_OF_THE_ONLY', 'ALLOW_TO_DO', 'HARD_WORKING_COMPOUND', 'MUST_HAVE', 'EM_ME', 'HOUSE_COMPOUNDS', 'IT_IS_2', 'SO_CALLED_HYPHEN', 'IN_ON_VACATION', 'CA_MOM_AND_POP', 'YEAR_OLD_PLURAL', 'MAC_OS', 'A_FEEDBACK', 'CA_FIRST_HAND', 'COMMA_COMPOUND_SENTENCE_2', 'SENT_START_THEM', 'HOLE_COMPOUNDS', 'AI', 'TOO_ADJECTIVE_TO', 'SUBJECT_MATTER_HYPHEN', 'ROLE_ROLL', 'LONG_COMPOUNDS', 'COMP_THAN', 'DO_ANYONE', 'NNS_THAT_AGREEMENT', 'WUD_LIKE', 'EVERYONE_OF', 'A_THANK_YOU', 'DROP_SHIP_HYPHEN', 'ASK_TO', 'I_IF', 'DOES_YOU', 'CLEAR_CUT_HYPHEN', 'CATCH_ALL_HYPHEN', 'NON_ACTION_CONTINUOUS', 'HELP_TO_FIND', 'THEY_WHERE', 'MERRIAM_WEBSTER', 'ETC_PERIOD', 'LOSE_LOSS', 'NOW', 'MISSING_PREPOSITION', 'GOTTA', 'NEAR_DEATH_HYPHEN', 'LED', 'SOME_WHAT_JJ', 'IF_IT_OK_FOR', 'CONFUSION_OF_OUR_OUT', 'A_INFORMATION', 'AM_I', 'HARDWARES', 'HAND_COMPOUNDS', 'TIK_TOK', 'PRP_VBG', 'FRISBEE', 'DONT_WHAT', 'AS_IS_VBG', 'TRADEMARK', 'CHRISTMAS_TIME', 'DT_DT', 'IN_WEEKDAY', 'MAKER_COMPOUNDS', 'MANY_NN_U', 'SUBJECT_VERB_AGREEMENT', 'U_RE', 'E_G', 'APOSTROPHE_IN_DAYS', 'TO_VB_ITS_NN', 'PHRASE_REPETITION', 'GUILTY_FOR_OF', 'BREAKER_COMPOUNDS', 'SPIDERMAN', 'POSSESSIVE_APOSTROPHE', 'SUFFER_OF_WITH', 'FOR_NOUN_SAKE', 'WHETHER', 'THIS_NNS_VB', 'COMPLAIN_FOR_ABOUT', 'A_BIT', 'MCDONALDS', 'A_UNCOUNTABLE', 'ONE_IN_THE_SAME', 'SPECIFIC_POSSESSIVE_APOSTROPHE', 'HYPOTHESIS_TYPOGRAPHY', 'ADVERB_OR_HYPHENATED_ADJECTIVE', 'CD_NN', 'UH_UH_COMMA', 'CROSS_COMPOUNDS', 'IDK', 'MILLION_DOLLAR_HYPHEN', 'OUTSIDE_OF', 'PRICE_PRIZE', 'NON_STANDARD_WORD', 'SUBJECT_MATTER', 'DATE_FUTURE_VERB_PAST', 'UP_TO_DATE_HYPHEN', 'CURRENCY_SPACE', 'HASNT_IRREGULAR_VERB', 'SHORT_COMPARATIVES', 'SEEKER_COMPOUNDS', 'FULL_WITH_OF', 'BREAKING_COMPOUNDS', 'SILL_STILL', 'AN_INVITE', 'DON_T_AREN_T'}\n"
     ]
    }
   ],
   "source": [
    "allErrors = set(simplifiedColErrors + simplifiedSportsErrors + simplifiedLegalErrors + simplifiedAdultErrors \n",
    "    + simplifiedMedErrors + simplifiedHsErrors + simplifiedBwayErrors + simplifiedPghErrors + simplifiedRantErrors\n",
    "   + simplifiedCcqErrors + simplifiedAnimeErrors + simplifiedEli5Errors + simplifiedCryptoErrors + \n",
    "   simplifiedLawyerErrors + simplifiedGamingErrors)\n",
    "print(allErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904ceca",
   "metadata": {},
   "source": [
    "This part is super messy, but I needed a way to get both the error and the count of that error in each subreddit into a list for each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55d6c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a bunch of lists\n",
    "col, sp, le, ad, med, hs, bway, pgh, rant, ccq, ani, eli5, cry, law, gam = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "# For each error, coint how many times that error occurs and append the error and the count to the list\n",
    "for x in allErrors:\n",
    "    \n",
    "    y = simplifiedColErrors.count(x)\n",
    "    col.append((x, y))\n",
    "    \n",
    "    y = simplifiedSportsErrors.count(x)\n",
    "    sp.append((x, y))\n",
    "    \n",
    "    y = simplifiedLegalErrors.count(x)\n",
    "    le.append((x, y))\n",
    "    \n",
    "    y = simplifiedAdultErrors.count(x)\n",
    "    ad.append((x, y))\n",
    "    \n",
    "    y = simplifiedMedErrors.count(x)\n",
    "    med.append((x, y))\n",
    "    \n",
    "    y = simplifiedHsErrors.count(x)\n",
    "    hs.append((x, y))\n",
    "    \n",
    "    y = simplifiedBwayErrors.count(x)\n",
    "    bway.append((x, y))\n",
    "    \n",
    "    y = simplifiedPghErrors.count(x)\n",
    "    pgh.append((x, y))\n",
    "    \n",
    "    y = simplifiedRantErrors.count(x)\n",
    "    rant.append((x, y))\n",
    "    \n",
    "    y = simplifiedCcqErrors.count(x)\n",
    "    ccq.append((x, y))\n",
    "    \n",
    "    y = simplifiedAnimeErrors.count(x)\n",
    "    ani.append((x, y))\n",
    "    \n",
    "    y = simplifiedEli5Errors.count(x)\n",
    "    eli5.append((x, y))\n",
    "    \n",
    "    y = simplifiedCryptoErrors.count(x)\n",
    "    cry.append((x, y))\n",
    "    \n",
    "    y = simplifiedLawyerErrors.count(x)\n",
    "    law.append((x, y))\n",
    "    \n",
    "    y = simplifiedGamingErrors.count(x)\n",
    "    gam.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33c47093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GONNA', 45),\n",
       " ('MISSING_HYPHEN', 49),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 55),\n",
       " ('IDK', 61),\n",
       " ('WANNA', 63),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 90),\n",
       " ('EN_COMPOUNDS', 112),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 126),\n",
       " ('WHITESPACE_RULE', 144),\n",
       " ('EN_CONTRACTION_SPELLING', 171),\n",
       " ('UPPERCASE_SENTENCE_START', 439),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 507),\n",
       " ('I_LOWERCASE', 961),\n",
       " ('MORFOLOGIK_RULE_EN_US', 1109),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1299)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.sort(key = lambda x: x[1])\n",
    "col[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42245bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 66),\n",
       " ('POSSESSIVE_APOSTROPHE', 92),\n",
       " ('ENGLISH_WORD_REPEAT_RULE', 94),\n",
       " ('CD_NN', 98),\n",
       " ('EN_COMPOUNDS', 102),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 106),\n",
       " ('EN_CONTRACTION_SPELLING', 134),\n",
       " ('SENTENCE_WHITESPACE', 140),\n",
       " ('UPPERCASE_SENTENCE_START', 142),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 174),\n",
       " ('EN_DIACRITICS_REPLACE', 188),\n",
       " ('EN_UNPAIRED_BRACKETS', 200),\n",
       " ('COMMA_COMPOUND_SENTENCE', 584),\n",
       " ('WHITESPACE_RULE', 2426),\n",
       " ('MORFOLOGIK_RULE_EN_US', 18742)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.sort(key = lambda x: x[1])\n",
    "sp[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eca005b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EN_UNPAIRED_BRACKETS', 51),\n",
       " ('ENGLISH_WORD_REPEAT_RULE', 53),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 65),\n",
       " ('MISSING_HYPHEN', 80),\n",
       " ('POSSESSIVE_APOSTROPHE', 83),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 118),\n",
       " ('EN_CONTRACTION_SPELLING', 143),\n",
       " ('EN_COMPOUNDS', 197),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 200),\n",
       " ('WHITESPACE_RULE', 282),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 372),\n",
       " ('UPPERCASE_SENTENCE_START', 391),\n",
       " ('I_LOWERCASE', 590),\n",
       " ('MORFOLOGIK_RULE_EN_US', 1112),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1776)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.sort(key = lambda x: x[1])\n",
    "le[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3939421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ENGLISH_WORD_REPEAT_RULE', 49),\n",
       " ('YEAR_OLD_HYPHEN', 57),\n",
       " ('ETC_PERIOD', 62),\n",
       " ('IDK', 64),\n",
       " ('DOUBLE_PUNCTUATION', 66),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 88),\n",
       " ('EN_CONTRACTION_SPELLING', 162),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 165),\n",
       " ('EN_COMPOUNDS', 169),\n",
       " ('UPPERCASE_SENTENCE_START', 361),\n",
       " ('WHITESPACE_RULE', 608),\n",
       " ('I_LOWERCASE', 714),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 720),\n",
       " ('MORFOLOGIK_RULE_EN_US', 1458),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1616)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.sort(key = lambda x: x[1])\n",
    "ad[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "278f952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HE_VERB_AGR', 29),\n",
       " ('EN_SPLIT_WORDS_HYPHEN', 29),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 35),\n",
       " ('I_LOWERCASE', 44),\n",
       " ('ETC_PERIOD', 44),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 48),\n",
       " ('EN_UNPAIRED_BRACKETS', 53),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 76),\n",
       " ('EN_CONTRACTION_SPELLING', 81),\n",
       " ('UPPERCASE_SENTENCE_START', 104),\n",
       " ('EN_COMPOUNDS', 115),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 180),\n",
       " ('WHITESPACE_RULE', 397),\n",
       " ('COMMA_COMPOUND_SENTENCE', 483),\n",
       " ('MORFOLOGIK_RULE_EN_US', 2399)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med.sort(key = lambda x: x[1])\n",
    "med[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ec0d187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT_IS', 44),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 54),\n",
       " ('EN_COMPOUNDS', 56),\n",
       " ('WANNA', 81),\n",
       " ('IDK', 85),\n",
       " ('GONNA', 89),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 104),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 127),\n",
       " ('WHITESPACE_RULE', 205),\n",
       " ('EN_CONTRACTION_SPELLING', 264),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 448),\n",
       " ('UPPERCASE_SENTENCE_START', 783),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1220),\n",
       " ('I_LOWERCASE', 1371),\n",
       " ('MORFOLOGIK_RULE_EN_US', 2089)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.sort(key = lambda x: x[1])\n",
    "hs[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41ef5894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('EN_UNPAIRED_BRACKETS', 35),\n",
       " ('HE_VERB_AGR', 39),\n",
       " ('ENGLISH_WORD_REPEAT_RULE', 39),\n",
       " ('EN_CONTRACTION_SPELLING', 46),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 51),\n",
       " ('EN_COMPOUNDS', 57),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 71),\n",
       " ('EN_DIACRITICS_REPLACE', 100),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 117),\n",
       " ('I_LOWERCASE', 144),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 149),\n",
       " ('WHITESPACE_RULE', 159),\n",
       " ('UPPERCASE_SENTENCE_START', 232),\n",
       " ('COMMA_COMPOUND_SENTENCE', 776),\n",
       " ('MORFOLOGIK_RULE_EN_US', 2428)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bway.sort(key = lambda x: x[1])\n",
    "bway[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d9d15cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MISSING_HYPHEN', 23),\n",
       " ('YEAR_OLD_HYPHEN', 24),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 33),\n",
       " ('EN_CONTRACTION_SPELLING', 34),\n",
       " ('DOUBLE_PUNCTUATION', 36),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 45),\n",
       " ('EN_COMPOUNDS', 56),\n",
       " ('I_LOWERCASE', 71),\n",
       " ('UPPERCASE_SENTENCE_START', 72),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 72),\n",
       " ('HE_VERB_AGR', 100),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 119),\n",
       " ('WHITESPACE_RULE', 141),\n",
       " ('COMMA_COMPOUND_SENTENCE', 420),\n",
       " ('MORFOLOGIK_RULE_EN_US', 1166)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgh.sort(key = lambda x: x[1])\n",
    "pgh[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c48bca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CANT', 64),\n",
       " ('IDK', 67),\n",
       " ('IT_IS', 76),\n",
       " ('DOUBLE_PUNCTUATION', 89),\n",
       " ('GONNA', 106),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 168),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 176),\n",
       " ('WHITESPACE_RULE', 247),\n",
       " ('EN_COMPOUNDS', 274),\n",
       " ('EN_CONTRACTION_SPELLING', 371),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 605),\n",
       " ('UPPERCASE_SENTENCE_START', 761),\n",
       " ('I_LOWERCASE', 1237),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1661),\n",
       " ('MORFOLOGIK_RULE_EN_US', 2300)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rant.sort(key = lambda x: x[1])\n",
    "rant[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdb07dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AM_I', 35),\n",
       " ('GONNA', 38),\n",
       " ('MISSING_HYPHEN', 49),\n",
       " ('EN_A_VS_AN', 64),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 78),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 86),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 123),\n",
       " ('UPPERCASE_SENTENCE_START', 126),\n",
       " ('EN_COMPOUNDS', 127),\n",
       " ('EN_CONTRACTION_SPELLING', 140),\n",
       " ('I_LOWERCASE', 303),\n",
       " ('WHITESPACE_RULE', 433),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 468),\n",
       " ('COMMA_COMPOUND_SENTENCE', 1039),\n",
       " ('MORFOLOGIK_RULE_EN_US', 1584)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccq.sort(key = lambda x: x[1])\n",
    "ccq[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6d55adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 61),\n",
       " ('ENGLISH_WORD_REPEAT_RULE', 64),\n",
       " ('ARROWS', 68),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 79),\n",
       " ('EN_UNPAIRED_BRACKETS', 82),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 87),\n",
       " ('EN_COMPOUNDS', 89),\n",
       " ('EN_CONTRACTION_SPELLING', 105),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 178),\n",
       " ('UPPERCASE_SENTENCE_START', 230),\n",
       " ('SENTENCE_WHITESPACE', 234),\n",
       " ('I_LOWERCASE', 319),\n",
       " ('WHITESPACE_RULE', 422),\n",
       " ('COMMA_COMPOUND_SENTENCE', 444),\n",
       " ('MORFOLOGIK_RULE_EN_US', 11050)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ani.sort(key = lambda x: x[1])\n",
    "ani[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "323dd82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IDK', 15),\n",
       " ('LETS_LET', 16),\n",
       " ('EN_UNPAIRED_BRACKETS', 18),\n",
       " ('HE_VERB_AGR', 21),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 21),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 23),\n",
       " ('UNIT_SPACE', 24),\n",
       " ('EN_COMPOUNDS', 29),\n",
       " ('EN_CONTRACTION_SPELLING', 48),\n",
       " ('WHITESPACE_RULE', 59),\n",
       " ('UPPERCASE_SENTENCE_START', 64),\n",
       " ('I_LOWERCASE', 78),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 89),\n",
       " ('COMMA_COMPOUND_SENTENCE', 219),\n",
       " ('MORFOLOGIK_RULE_EN_US', 632)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.sort(key = lambda x: x[1])\n",
    "eli5[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fda5462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SENTENCE_WHITESPACE', 69),\n",
       " ('ENGLISH_WORD_REPEAT_RULE', 71),\n",
       " ('DOUBLE_PUNCTUATION', 78),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 107),\n",
       " ('EN_CONTRACTION_SPELLING', 138),\n",
       " ('EN_UNPAIRED_BRACKETS', 148),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 153),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 172),\n",
       " ('UPPERCASE_SENTENCE_START', 216),\n",
       " ('EN_COMPOUNDS', 216),\n",
       " ('I_LOWERCASE', 243),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 273),\n",
       " ('COMMA_COMPOUND_SENTENCE', 858),\n",
       " ('WHITESPACE_RULE', 2555),\n",
       " ('MORFOLOGIK_RULE_EN_US', 6229)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cry.sort(key = lambda x: x[1])\n",
    "cry[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "116d7e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POSSESSIVE_APOSTROPHE', 34),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 39),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 40),\n",
       " ('MISSING_HYPHEN', 41),\n",
       " ('AM_I', 45),\n",
       " ('EN_CONTRACTION_SPELLING', 55),\n",
       " ('HE_VERB_AGR', 58),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 90),\n",
       " ('EN_COMPOUNDS', 129),\n",
       " ('WHITESPACE_RULE', 157),\n",
       " ('UPPERCASE_SENTENCE_START', 159),\n",
       " ('I_LOWERCASE', 192),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 426),\n",
       " ('COMMA_COMPOUND_SENTENCE', 673),\n",
       " ('MORFOLOGIK_RULE_EN_US', 892)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "law.sort(key = lambda x: x[1])\n",
    "law[1011:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9323055f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('POKEMON', 39),\n",
       " ('HE_VERB_AGR', 42),\n",
       " ('DOUBLE_PUNCTUATION', 44),\n",
       " ('SENT_START_CONJUNCTIVE_LINKING_ADVERB_COMMA', 56),\n",
       " ('EN_COMPOUNDS', 90),\n",
       " ('COMMA_PARENTHESIS_WHITESPACE', 122),\n",
       " ('ENGLISH_WORD_REPEAT_BEGINNING_RULE', 124),\n",
       " ('COMMA_COMPOUND_SENTENCE_2', 126),\n",
       " ('EN_SPECIFIC_CASE', 168),\n",
       " ('EN_CONTRACTION_SPELLING', 181),\n",
       " ('WHITESPACE_RULE', 234),\n",
       " ('UPPERCASE_SENTENCE_START', 245),\n",
       " ('I_LOWERCASE', 398),\n",
       " ('COMMA_COMPOUND_SENTENCE', 619),\n",
       " ('MORFOLOGIK_RULE_EN_US', 2568)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gam.sort(key = lambda x: x[1])\n",
    "gam[1011:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b763f2c",
   "metadata": {},
   "source": [
    "**Top Errors for each subreddit**:\n",
    "\n",
    "*Legal Advice* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Adulting* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Medicine* -  MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Highschool* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Broadway* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Pittsburgh* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Rant* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Ccq* -  MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Anime* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Eli5* -  MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*College* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Sports* -  MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Cryptocurrency* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Lawyer Talk* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Gaming* - MORFOLOGIK_RULE_EN_US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b21ad",
   "metadata": {},
   "source": [
    "MORFOLOGIK_RULE_EN_US occurs as the top error in 12/15 of the subreddits.\n",
    "\n",
    "Does this really tell us much, though? Most, if not at all, of the MORFOLOGIK_RULE_EN_US errors are due to spelling errors, and while spelling is important to grammaticality, typos don't really tell me much, especially when 12/15 subreddits top error is MORFOLOGIK_RULE_EN_US. The Anime subreddit had the most MORFOLOGIK_RULE_EN_US errors, with 11050 errors. However, could this be due to names of anime characters that the parser does not recognize? Therefore, while I will consider this in my further analysis, I am also curious as to how this holds for the second most common errors in each subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ccc27f",
   "metadata": {},
   "source": [
    " **Second Top Errors for each subreddit**:\n",
    " \n",
    "*Legal Advice* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Adulting* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Medicine* -  COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Highschool* - I_LOWERCASE\n",
    "\n",
    "*Broadway* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Pittsburgh* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Rant* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Ccq* -  COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Anime* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Eli5* -  COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*College* - MORFOLOGIK_RULE_EN_US\n",
    "\n",
    "*Sports* -  WHITESPACE_RULE\n",
    "\n",
    "*Cryptocurrency* - WHITESPACE_RULE\n",
    "\n",
    "*Lawyer Talk* - COMMA_COMPOUND_SENTENCE\n",
    "\n",
    "*Gaming* - COMMA_COMPOUND_SENTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9fe17",
   "metadata": {},
   "source": [
    "If the subreddit's top error was MORFOLOGIK_RULE_EN_US before, it turned into COMMA_COMPOUND_SENTENCE for the majority of the subreddits. If it was COMMA_COMPOUND_SENTENCE, then it became MORFOLOGIK_RULE_EN_US. COMMA_COMPOUND_SENTENCE relates to a missing comma for compound sentences, which is very interesting. A lot of people do tend to write run-on sentences online, ignoring the rules they would follow in an academic paper, for example. However, it is interesting that the two top ignored grammaticality areas when typing online are typos and forgetting a comma for a compound sentence. The other cases were the whitespace rule, and I lowercase. Whitespace rule relates to how much whitespace is in between words, sentences, etc. I lower case relates to putting a lowercase i instead of an uppercase one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d276ea4",
   "metadata": {},
   "source": [
    "In fact, if you look at the top three highest occurring errors for any subreddit, it will be some combination of COMMA_COMPOUND_SENTENCE, WHITESPACE_RULE, MORFOLOGIK_RULE_EN_US, and ENGLISH_WORD_REPEAT_BEGINNING_RULE. Other top 15 errors, that may not occur in other subreddits, will be interesting to explore.\n",
    "\n",
    "For example, it seems like there is a \"POKEMON\" error in the gaming subreddit, which I assume is due to the last of accent in the word. There are also \"WANNA\" and \"GONNA\" errors, which I assume is due to the use of the \"wanna\" and \"gonna\", are found in different subreddits but not all. Specifically, some contain both \"WANNA\" and \"GONNA\", some contain one or the other, or some contain neither. There is also an \"IDK\" error that occurs in some, but not all, of subreddits. These errors are meaningful to me, because they do show a difference in writing among different subreddits. However, I want to explore these errors more, get more detail about them, and see if I can find even deeper analysis of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86425425",
   "metadata": {},
   "source": [
    "## Analysis To Be Continued"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
